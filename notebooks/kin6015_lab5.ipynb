{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plnu-biomechanics/kin6015/blob/main/notebooks/kin6015_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.pointloma.edu/sites/default/files/styles/basic_page/public/images/PLNU_Biomechanics_Lab_green_yellowSD_HiRes.png\" width=400>\n",
        "\n",
        "## **KIN 6015 Biomechanical Basis of Human Movement**\n",
        "Instructor: Arnel Aguinaldo, PhD\n",
        "\n",
        "**Lab 5 Data Processing**\n",
        "\n",
        "In this lab, throwing data was collected using markerless motion capture and an instrumented mound in 4 conditions: full, feet stationary, locked hips-shoulders, and kneeling. Inverse kinematics (IK) and kinetics via inverse dynamics were estimated using Visual3D. The data were then exported as text (*.txt) files and uploaded to the class repository in the lab's [GitHub](https://github.com/plnu-biomechanics).\n",
        "\n",
        "To further process the data for this lab, follow the steps in this **Colab notebook**, which contains instructions and sample code on how to wrangle and analyze the data.\n"
      ],
      "metadata": {
        "id": "BlhMHK7IAzfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create your own Colab Notebook\n",
        "\n",
        "1. Go to **File -> New notebook in Drive** to open a new notebook in your Python environment:<br>\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/file_notebook.png\" width=450>\n",
        "\n",
        "2. Rename your Colab notebook using this naming format: **lastname_group_lab#.ipynb** (e.g., \"aguinaldo_targaryen_lab1.ipynb\")\n",
        "3. Click on the **+ Code** option above to insert a new code cell: <br>\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/addcode.png\" width=280>\n",
        "\n",
        "4. The data you will parse and analyze for this lab will be copied from the lab's GitHub and temporarily stored in your Colab's runtime directory, which can be accessed by clicking on the folder icon in the left menu:<br>\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/colab_folder.png\" width=400>\n",
        "\n",
        "5. Copy the following lines of code to import the packages needed for this analysis and to load the data files into your working directory. Be sure to update the `GROUP` variable with your group's name. **Note**: These files are \"runtime\" access only, meaning they are only temporarily stored in your working directory and show up when your notebook is in session. However, the following code cell allows you to clone the zipped files to the working directory each time it is executed.\n"
      ],
      "metadata": {
        "id": "j1Gi2rPXJDOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A3-y3-4GKN-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b36ae4-997e-4cb4-9526-ec6d9089303e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files in lab directory:\n",
            "['Stark_Lab5_HipShoulderLock.txt', '__MACOSX', 'Stark_Lab5_FeetStationary.txt', 'Stark_Lab5_Full.txt', 'Stark_Lab5_Kneeling.txt', 'spring2026_lab5_stark.zip']\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STUDENT INPUT (edit only this line; case-sensitive)\n",
        "# --------------------------------------------------\n",
        "GROUP = \"stark\"   # e.g., \"targaryen\", \"stark\", \"lannister\", \"martell\", \"greyjoy\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Configuration (do NOT edit below)\n",
        "# These lines create a directory for this lab in your\n",
        "# Colab working directory.\n",
        "# --------------------------------------------------\n",
        "zip_dir = \"kin6015/lab5\"\n",
        "os.makedirs(zip_dir, exist_ok=True)\n",
        "\n",
        "zip_filename = f\"spring2026_lab5_{GROUP}.zip\"\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/\"\n",
        "    \"plnu-biomechanics/kin6015/main/\"\n",
        "    f\"labs/{zip_filename}\"\n",
        ")\n",
        "\n",
        "zip_path = os.path.join(zip_dir, zip_filename)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Download zip file\n",
        "# --------------------------------------------------\n",
        "urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Extract contents from the zipped file\n",
        "# --------------------------------------------------\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(zip_dir)\n",
        "\n",
        "print(\"Extracted files in lab directory:\")\n",
        "print(os.listdir(zip_dir))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a75efdc"
      },
      "source": [
        "## ðŸ”„ Data Parsing\n",
        "\n",
        "You can add your own code below to parse the data needed for this lab by either using GenAI (e.g., Gemini, ChatGPT) or simply copying the prepared code below.\n",
        "\n",
        "### âœ‹ GenAI prompt:\n",
        "Parse the data from the text files for the discrete metrics in the columns 2-6 in row 6 (index 5). The column headings are found in row 2: BALL_RELEASE_SPEED_mph, Pelvis_to_Trunk_ms, Max_Elbow_Varus_Torque_Nm, Back_Leg_GRF_Nm, Lead_Leg_GRF_Nm. Be sure to organize them by throwing conditions, which are denoted in the filename as metadata after last `_` character before the `.txt` extension.\n",
        "\n",
        "### âœ… Pre-compiled code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91be9214",
        "outputId": "8f862215-a63b-419f-de6d-9c243a7c0272"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re # Import the regular expression module\n",
        "import numpy as np # Import numpy for np.nan\n",
        "\n",
        "def parse_txt_file(filepath):\n",
        "  \"\"\"Reads a text file, extracts specific discrete metrics from row 6 (index 5)\n",
        "  and assigns predefined column names.\n",
        "\n",
        "  Args:\n",
        "    filepath (str): The full path to the text file.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: A DataFrame containing the extracted discrete data for a single trial.\n",
        "  \"\"\"\n",
        "  with open(filepath, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "  # Ensure there are at least 6 lines for data in row 6 (index 5)\n",
        "  if len(lines) < 6:\n",
        "      raise ValueError(f\"File {os.path.basename(filepath)} has too few lines to parse correctly for discrete metrics.\")\n",
        "\n",
        "  # The row is row 6, which is lines[5].\n",
        "  data_row_str = lines[5].strip()\n",
        "  data_values_all = data_row_str.split('\\t')\n",
        "\n",
        "  # Determine 'condition' from filename first\n",
        "  filename = os.path.basename(filepath)\n",
        "  condition = 'unknown'\n",
        "  if 'Full' in filename:\n",
        "    condition = 'full'\n",
        "  elif 'FeetStationary' in filename:\n",
        "    condition = 'feet_stationary'\n",
        "  elif 'HipShoulderLock' in filename:\n",
        "    condition = 'locked_hips_shoulders'\n",
        "  elif 'Kneeling' in filename:\n",
        "    condition = 'kneeling'\n",
        "\n",
        "  # Predefined column names for the output DataFrame\n",
        "  column_names = [\n",
        "      'BALL_RELEASE_SPEED_mph',\n",
        "      'Pelvis_to_Trunk_ms',\n",
        "      'Max_Elbow_Varus_Torque_Nm',\n",
        "      'Back_Leg_GRF_Nm',\n",
        "      'Lead_Leg_GRF_Nm'\n",
        "  ]\n",
        "\n",
        "  # Dictionary to store parsed values for the current file\n",
        "  parsed_values = {}\n",
        "\n",
        "  if condition == 'kneeling':\n",
        "    # Kneeling files only have the first 3 data columns (corresponding to indices 1, 2, 3 of data_values_all)\n",
        "    # Check if data_values_all has enough elements for these 3\n",
        "    if len(data_values_all) < 4: # Need at least index 3 (0-indexed)\n",
        "        raise ValueError(f\"File {filename} (kneeling) does not have enough columns for the first 3 metrics in row 6 (index 5). Expected at least 4 split elements, got {len(data_values_all)}.\")\n",
        "\n",
        "    parsed_values[column_names[0]] = data_values_all[1]\n",
        "    parsed_values[column_names[1]] = data_values_all[2]\n",
        "    parsed_values[column_names[2]] = data_values_all[3]\n",
        "    parsed_values[column_names[3]] = np.nan # Missing for kneeling condition\n",
        "    parsed_values[column_names[4]] = np.nan # Missing for kneeling condition\n",
        "  else:\n",
        "    # Other conditions expect all 5 metrics (columns 2-6, i.e., indices 1-5 of data_values_all)\n",
        "    # Check if data_values_all has enough elements for these 5\n",
        "    if len(data_values_all) < 6: # Need at least index 5 (0-indexed)\n",
        "        raise ValueError(f\"File {filename} ({condition}) does not have enough columns to extract all 5 metrics (cols 2-6) in row 6 (index 5). Expected at least 6 split elements, got {len(data_values_all)}.\")\n",
        "\n",
        "    parsed_values[column_names[0]] = data_values_all[1]\n",
        "    parsed_values[column_names[1]] = data_values_all[2]\n",
        "    parsed_values[column_names[2]] = data_values_all[3]\n",
        "    parsed_values[column_names[3]] = data_values_all[4]\n",
        "    parsed_values[column_names[4]] = data_values_all[5]\n",
        "\n",
        "  # Create a DataFrame from the dictionary\n",
        "  df = pd.DataFrame([parsed_values])\n",
        "\n",
        "  # Convert numeric columns to appropriate data types\n",
        "  for col in df.columns:\n",
        "    try:\n",
        "      df[col] = pd.to_numeric(df[col])\n",
        "    except ValueError:\n",
        "      pass # Keep as string if not numeric\n",
        "\n",
        "  df['condition'] = condition\n",
        "  df['filename'] = filename\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "# 1. Get the list of .txt files from the working directory\n",
        "zip_dir = \"kin6015/lab5\"\n",
        "all_files = os.listdir(zip_dir)\n",
        "txt_files = [f for f in all_files if f.endswith('.txt')]\n",
        "\n",
        "# 2. Initialize an empty list to store DataFrames\n",
        "allmetrics = []\n",
        "\n",
        "# 3. Iterate through each .txt file and apply the parse_txt_file function\n",
        "for filename in txt_files:\n",
        "    filepath = os.path.join(zip_dir, filename)\n",
        "    try:\n",
        "        df = parse_txt_file(filepath)\n",
        "        allmetrics.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "# Check if allmetrics is not empty before concatenating\n",
        "if allmetrics:\n",
        "    # 4. Concatenate all DataFrames in allmetrics into a single DataFrame\n",
        "    combined_discretes_df = pd.concat(allmetrics, ignore_index=True)\n",
        "\n",
        "    # 5. Create discretes_df by selecting the specified columns\n",
        "    discretes_df = combined_discretes_df[[\n",
        "            'filename',\n",
        "            'condition',\n",
        "            'BALL_RELEASE_SPEED_mph',\n",
        "            'Pelvis_to_Trunk_ms',\n",
        "            'Max_Elbow_Varus_Torque_Nm',\n",
        "            'Back_Leg_GRF_Nm',\n",
        "            'Lead_Leg_GRF_Nm'\n",
        "        ]]\n",
        "\n",
        "    # 6. Display the head of discretes_df\n",
        "    print(\"\\nHead of discretes_df:\")\n",
        "    print(discretes_df.head())\n",
        "else:\n",
        "    print(\"No dataframes were successfully parsed and added to allmetrics.\")\n",
        "    discretes_df = pd.DataFrame() # Initialize an empty DataFrame if no files were processed successfully\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Head of discretes_df:\n",
            "                         filename              condition  \\\n",
            "0  Stark_Lab5_HipShoulderLock.txt  locked_hips_shoulders   \n",
            "1   Stark_Lab5_FeetStationary.txt        feet_stationary   \n",
            "2             Stark_Lab5_Full.txt                   full   \n",
            "3         Stark_Lab5_Kneeling.txt               kneeling   \n",
            "\n",
            "   BALL_RELEASE_SPEED_mph  Pelvis_to_Trunk_ms  Max_Elbow_Varus_Torque_Nm  \\\n",
            "0                    41.9            -6.66666                   55.15063   \n",
            "1                    65.0            -6.66667                  126.26793   \n",
            "2                    71.4            13.33337                  132.84457   \n",
            "3                    50.4            -6.66666                   96.09460   \n",
            "\n",
            "   Back_Leg_GRF_Nm  Lead_Leg_GRF_Nm  \n",
            "0        610.69495        394.08441  \n",
            "1        898.33899       1112.17932  \n",
            "2       1201.27673       1729.09155  \n",
            "3              NaN              NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ‘ Complete the Analysis\n",
        "\n",
        "After you've completed this lab's analysis, do the following:\n",
        "\n",
        "1. Copy and paste the above plots to your lab report\n",
        "2. Export the tables using the code in the following cell. You can download them from your runtime directory onto your own computer.\n",
        "2. Share your Colab notebook with me\n",
        "3. Copy the link to your notebook and submit it along with your report for this lab on Canvas"
      ],
      "metadata": {
        "id": "W0vQo8A1VFOX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08cf6c31",
        "outputId": "b9f9f3a1-82fd-4b8c-d40c-ff64b1b9cf85"
      },
      "source": [
        "import os\n",
        "\n",
        "# Construct the full file path for the CSV file\n",
        "output_filepath = os.path.join(zip_dir, f\"{GROUP}_discretes_for_analysis_lab5.csv\")\n",
        "\n",
        "# Export the DataFrame to CSV\n",
        "discretes_df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"'discretes_df' exported to '{output_filepath}'\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'discretes_df' exported to 'kin6015/lab5/stark_discretes_for_analysis_lab5.csv'\n"
          ]
        }
      ]
    }
  ]
}