{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plnu-biomechanics/kin6015/blob/main/notebooks/kin6015_lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.pointloma.edu/sites/default/files/styles/basic_page/public/images/PLNU_Biomechanics_Lab_green_yellowSD_HiRes.png\" width=400>\n",
        "\n",
        "## **KIN 6015 Biomechanical Basis of Human Movement**\n",
        "Instructor: Arnel Aguinaldo, PhD\n",
        "\n",
        "**Lab 6 Data Processing**\n",
        "\n",
        "In this lab, CMJ and SJ data was collected with muscle activation patterns for the rectus femoris, hamstring, and gastroc-soleus complex using surface electromyography (EMG). Inverse kinematics (IK) and kinetics via inverse dynamics as well as rectified EMG were estimated using Visual3D. The data were then exported as text (*.txt) files and uploaded to the class repository in the lab's [GitHub](https://github.com/plnu-biomechanics).\n",
        "\n",
        "To further process the data for this lab, follow the steps in this **Colab notebook**, which contains instructions and sample code on how to wrangle and analyze the data.\n"
      ],
      "metadata": {
        "id": "BlhMHK7IAzfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create your own Colab Notebook\n",
        "\n",
        "1. Go to **File -> New notebook in Drive** to open a new notebook in your Python environment:<br>\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/file_notebook.png\" width=450>\n",
        "\n",
        "2. Rename your Colab notebook using this naming format: **lastname_group_lab#.ipynb** (e.g., \"aguinaldo_targaryen_lab1.ipynb\")\n",
        "3. Click on the **+ Code** option above to insert a new code cell: <br>\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/addcode.png\" width=280>\n",
        "\n",
        "4. The data you will parse and analyze for this lab will be copied from the lab's GitHub and temporarily stored in your Colab's runtime directory, which can be accessed by clicking on the folder icon in the left menu:<br>\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/colab_folder.png\" width=400>\n",
        "\n",
        "5. Copy the following lines of code to import the packages needed for this analysis and to load the data files into your working directory. Be sure to update the `GROUP` variable with your group's name. **Note**: These files are \"runtime\" access only, meaning they are only temporarily stored in your working directory and show up when your notebook is in session. However, the following code cell allows you to clone the zipped files to the working directory each time it is executed.\n"
      ],
      "metadata": {
        "id": "j1Gi2rPXJDOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3-y3-4GKN-l"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STUDENT INPUT (edit only this line; case-sensitive)\n",
        "# --------------------------------------------------\n",
        "GROUP = \"stark\"   # e.g., \"targaryen\", \"stark\", \"lannister\", \"martell\", \"greyjoy\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Configuration (do NOT edit below)\n",
        "# These lines create a directory for this lab in your\n",
        "# Colab working directory.\n",
        "# --------------------------------------------------\n",
        "zip_dir = \"kin6015/lab6\"\n",
        "os.makedirs(zip_dir, exist_ok=True)\n",
        "\n",
        "zip_filename = f\"spring2026_lab6_{GROUP}.zip\"\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/\"\n",
        "    \"plnu-biomechanics/kin6015/main/\"\n",
        "    f\"labs/{zip_filename}\"\n",
        ")\n",
        "\n",
        "zip_path = os.path.join(zip_dir, zip_filename)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Download zip file\n",
        "# --------------------------------------------------\n",
        "urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Extract contents from the zipped file\n",
        "# --------------------------------------------------\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(zip_dir)\n",
        "\n",
        "print(\"Extracted files in lab directory:\")\n",
        "print(os.listdir(zip_dir))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a75efdc"
      },
      "source": [
        "## ðŸ”„ Data Parsing\n",
        "\n",
        "You can add your own code below to parse the data needed for this lab by either using GenAI (e.g., Gemini, ChatGPT) or simply copying the prepared code below.\n",
        "\n",
        "### âœ‹ GenAI prompt:\n",
        "Parse the data from the text files so that they are two unique data frames, one for the timeseries data and another for the discrete metrics. The timeseries data (101 frames) are in the `Vertical_GRF`, `HAMSTRING`, `RECTUS_FEM` and `GASTROC` columns and the discrete metrics are in the remaining columns in row 6 (index 5). Be sure to organize them by CMJ and SJ conditions, which are denoted in the filename as metadata.\n",
        "\n",
        "### âœ… Pre-compiled code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91be9214"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re # Import the regular expression module\n",
        "\n",
        "def parse_txt_file(filepath):\n",
        "  \"\"\"Reads a text file, extracts variable names and time-series data, identifies the condition,\n",
        "  and returns a pandas DataFrame.\n",
        "\n",
        "  Args:\n",
        "    filepath (str): The full path to the text file.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: A DataFrame containing the extracted data, condition, and filename.\n",
        "  \"\"\"\n",
        "  with open(filepath, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "  # Check if file has enough lines for the expected structure\n",
        "  if len(lines) < 6: # Need at least 6 lines (5 header, 1 data)\n",
        "      raise ValueError(f\"File {os.path.basename(filepath)} has too few lines to parse correctly.\")\n",
        "\n",
        "  # Extract variable names from the 2nd line (index 1), using tab as delimiter\n",
        "  variable_names = lines[1].strip().split('\\t')\n",
        "\n",
        "  # Read time-series data starting from the 6th line (index 5), using tab as delimiter\n",
        "  data_lines = [line.strip().split('\\t') for line in lines[5:] if line.strip()]\n",
        "\n",
        "  # Ensure data_lines is not empty before checking lengths\n",
        "  if not data_lines:\n",
        "      raise ValueError(f\"No data found in file {os.path.basename(filepath)} after header.\")\n",
        "\n",
        "  # Dynamically add 'Frame' if data rows have one more column than variable names\n",
        "  if len(data_lines[0]) == len(variable_names) + 1:\n",
        "      variable_names.insert(0, 'Frame')\n",
        "\n",
        "  # Ensure variable names and data lines match in length AFTER potential adjustment\n",
        "  if len(variable_names) != len(data_lines[0]):\n",
        "      raise ValueError(f\"Column name count ({len(variable_names)}) does not match data column count ({len(data_lines[0])}) in file {os.path.basename(filepath)}.\")\n",
        "\n",
        "  # Create DataFrame\n",
        "  df = pd.DataFrame(data_lines, columns=variable_names)\n",
        "\n",
        "    # Rename 'MAX_R_GRF_MEAN' to 'Vertical GRF' if it exists\n",
        "  if 'GRFz' in df.columns:\n",
        "    df = df.rename(columns={'GRFz': 'Vertical_GRF'})\n",
        "\n",
        "  # Convert numeric columns to appropriate data types\n",
        "  for col in df.columns:\n",
        "    try:\n",
        "      df[col] = pd.to_numeric(df[col])\n",
        "    except ValueError:\n",
        "      pass # Keep as string if not numeric\n",
        "\n",
        "  # Determine 'condition' from filename\n",
        "  filename = os.path.basename(filepath)\n",
        "  if 'CMJ' in filename:\n",
        "    condition = 'CMJ'\n",
        "  elif 'SJ' in filename:\n",
        "    condition = 'SJ'\n",
        "  else:\n",
        "    condition = 'unknown'\n",
        "\n",
        "  # Add 'condition' column\n",
        "  df['condition'] = condition\n",
        "\n",
        "  # Add 'filename' column\n",
        "  df['filename'] = filename\n",
        "\n",
        "  return df\n",
        "\n",
        "print(\"Updated function 'parse_txt_file' to handle tab-delimited data, correct line indexing, dynamic 'Frame' column, case-sensitive condition check, import 're' module, rename 'Right Ankle Angles' column, and remove '_MEAN'/'_Mean' suffixes.\")\n",
        "\n",
        "\n",
        "# 1. Get the list of .txt files from the working directory\n",
        "zip_dir = \"kin6015/lab6\"\n",
        "all_files = os.listdir(zip_dir)\n",
        "txt_files = [f for f in all_files if f.endswith('.txt')]\n",
        "\n",
        "# 2. Initialize an empty list to store DataFrames\n",
        "all_dataframes = []\n",
        "\n",
        "# 3. Iterate through each .txt file and apply the parse_txt_file function\n",
        "for filename in txt_files:\n",
        "    filepath = os.path.join(zip_dir, filename)\n",
        "    try:\n",
        "        df = parse_txt_file(filepath)\n",
        "        all_dataframes.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "# 4. Concatenate all DataFrames into a single combined_intermediate_df\n",
        "if all_dataframes:\n",
        "    combined_intermediate_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "    print(\"Combined_intermediate_df created successfully.\")\n",
        "else:\n",
        "    combined_intermediate_df = pd.DataFrame()\n",
        "    print(\"No dataframes to concatenate, combined_intermediate_df is empty.\")\n",
        "\n",
        "# Multiply specified columns by 100, if needed\n",
        "# combined_intermediate_df['HAMSTRING'] = combined_intermediate_df['HAMSTRING'] * 100\n",
        "# combined_intermediate_df['RECTUS_FEM'] = combined_intermediate_df['RECTUS_FEM'] * 100\n",
        "# combined_intermediate_df['GASTROC'] = combined_intermediate_df['GASTROC'] * 100\n",
        "\n",
        "# 5. Create timeseries_df\n",
        "timeseries_df = combined_intermediate_df[\n",
        "    [\n",
        "        'Frame',\n",
        "        'Vertical_GRF',\n",
        "        'HAMSTRING',\n",
        "        'RECTUS_FEM',\n",
        "        'GASTROC',\n",
        "        'condition',\n",
        "        'filename'\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "# 6. Create discretes_df\n",
        "discretes_df = combined_intermediate_df[combined_intermediate_df['Frame'] == 1][\n",
        "    [\n",
        "        'filename',\n",
        "        'condition',\n",
        "        'Jump_Height_cm',\n",
        "        'PEAK_GRFz'\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "# 7. Display the head of timeseries_df\n",
        "print(\"\\nHead of timeseries_df:\")\n",
        "print(timeseries_df.head())\n",
        "\n",
        "# 8. Display the head of discretes_df\n",
        "print(\"\\nHead of discretes_df:\")\n",
        "print(discretes_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ˆ Statistical Parametric Mapping (SPM)\n",
        "\n",
        "Statistical parametric mapping (SPM) uses Random Field Theory to make statistical inferences about normalized sets of biomechanical variables across time (1D). It can be used to compare kinematic, kinetic, or EMG time-series curves using traditional NHST statistical tests such as a t-test, ANOVA, and linear regression. In this course, we will be using the package, `spm1d`, in [Python](https://spm1d.org/index.html) to perform some basic SPM analyses.\n",
        "\n",
        "Before we perform the SPM analyses, we must prepare the time-series data so that they can be properly structured for SPM. The process of preparing and organizing raw data into usable data is known as **data wrangling**. For SPM, the time-series data must be structured so that each variable for each condition (i.e., conventional, sumo) is its own data file, where the data is organized in a *JxQ* array:\n",
        "\n",
        "* J = number of trials or subjects\n",
        "* Q = number of frames or time points (i.e., 100)\n",
        "\n",
        "The following code wrangles the time-series data in these arrays and stores them into individual data frames for subsequent SPM analysis. This will allow the `spm1d` package to average the data at each frame across all trials in a process known as **ensemble averaging** where the solid thick lines represent the mean and the semi-transparent bands represent the +/- 1 standard deviation:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/plnu-biomechanics/kin6015/main/notebooks/images/SPM_plot_sample.png\">\n",
        "\n",
        "This two-line cell installs the `spm1d` library in runtime environment and confirms installation."
      ],
      "metadata": {
        "id": "Ha5t7miYDRlX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c3262c5"
      },
      "source": [
        "!pip install spm1d\n",
        "print(\"spm1d library installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Wrangling\n",
        "\n",
        "Now the data from the `timeseries_df` data frame must be extracted and formatted so that they can be properly analyzed using SPM. This is a part of data wrangling and is a common and important part of the data parsing process.\n",
        "\n",
        "### âœ‹ GenAI prompt:\n",
        "Generate code that wrangles the time-series data into a SPM friendly format for the time-series data in this lab. Each data array should be in the shape of j x q where j = 5 (conditions) and q = 101 (frames). There would be a total of 8 separate data arrays:\n",
        "\n",
        "*   verticalgrf_CMJ\n",
        "*   verticalgrf_SJ\n",
        "*   hamstring_CMJ\n",
        "*   hamstring_SJ\n",
        "*   rectus_CMJ\n",
        "*   rectus_SJ\n",
        "*   gastroc_CMJ\n",
        "*   gastroc_SJ\n",
        "\n",
        "\n",
        "### âœ… Pre-compiled code:"
      ],
      "metadata": {
        "id": "0vNdRwD2ZfGl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22fff73a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# ================\n",
        "# DATA WRANGLING\n",
        "# ================\n",
        "\n",
        "# This code chunk wrangles time-series data into respective arrays for SPM analysis\n",
        "\n",
        "# Recreate timeseries_df with .copy() to avoid SettingWithCopyWarning\n",
        "timeseries_df = combined_intermediate_df[\n",
        "    [\n",
        "        'Frame',\n",
        "        'Vertical_GRF',\n",
        "        'HAMSTRING',\n",
        "        'RECTUS_FEM',\n",
        "        'GASTROC',\n",
        "        'condition',\n",
        "        'filename'\n",
        "        ]\n",
        "    ].copy()\n",
        "\n",
        "# Add 'trial_id' column by cleaning the 'filename'\n",
        "timeseries_df['trial_id'] = timeseries_df['filename'].str.replace('_CMJ', '', regex=False)\n",
        "timeseries_df['trial_id'] = timeseries_df['trial_id'].str.replace('_SJ', '', regex=False)\n",
        "timeseries_df['trial_id'] = timeseries_df['trial_id'].str.replace('.txt', '', regex=False)\n",
        "\n",
        "print(\"Head of timeseries_df with new 'trial_id' column:\")\n",
        "display(timeseries_df)\n",
        "\n",
        "print(\"\\nUnique trial IDs:\")\n",
        "print(timeseries_df.head())\n",
        "\n",
        "print(\"\\nUnique trial IDs:\")\n",
        "print(timeseries_df['trial_id'].unique())\n",
        "\n",
        "# export CSV file for inspection (comment if not needed)\n",
        "# timeseries_df.to_csv('timeseries_data.csv', index=False)\n",
        "# print(\"timeseries_df exported to 'timeseries_data.csv'\")\n",
        "\n",
        "# Filter for both conditions\n",
        "CMJ_df = timeseries_df[timeseries_df['condition'] == 'CMJ']\n",
        "SJ_df = timeseries_df[timeseries_df['condition'] == 'SJ']\n",
        "\n",
        "# Prepare data for GRF (CMJ)\n",
        "verticalgrf_CMJ_data = CMJ_df.pivot(index='trial_id', columns='Frame', values='Vertical_GRF').values\n",
        "\n",
        "# Prepare data for Verticalulsive (SJ)\n",
        "verticalgrf_SJ_data = SJ_df.pivot(index='trial_id', columns='Frame', values='Vertical_GRF').values\n",
        "\n",
        "# Prepare data for Hamstring EMG (CMJ)\n",
        "hamstring_CMJ_data = CMJ_df.pivot(index='trial_id', columns='Frame', values='HAMSTRING').values\n",
        "\n",
        "# Prepare data for Hamstring EMG (SJ)\n",
        "hamstring_SJ_data = SJ_df.pivot(index='trial_id', columns='Frame', values='HAMSTRING').values\n",
        "\n",
        "# Prepare data for Rectus EMG (CMJ)\n",
        "rectus_CMJ_data = CMJ_df.pivot(index='trial_id', columns='Frame', values='RECTUS_FEM').values\n",
        "\n",
        "# Prepare data for Rectus EMG (SJ)\n",
        "rectus_SJ_data = SJ_df.pivot(index='trial_id', columns='Frame', values='RECTUS_FEM').values\n",
        "\n",
        "# Prepare data for Gastroc EMG (CMJ)\n",
        "gastroc_CMJ_data = CMJ_df.pivot(index='trial_id', columns='Frame', values='GASTROC').values\n",
        "\n",
        "# Prepare data for Rectus EMG (SJ)\n",
        "gastroc_SJ_data = SJ_df.pivot(index='trial_id', columns='Frame', values='GASTROC').values\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "# FIND CMJ TAKE-OFF TIME\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "\n",
        "toe_off_frames = [] # Initialize an empty list to store the toe-off frame for each CMJ trial\n",
        "\n",
        "num_trials = len(verticalgrf_CMJ_data) # Number of trials (rows) in the data\n",
        "total_frames = verticalgrf_CMJ_data.shape[1] # Total number of frames (columns) in the data\n",
        "# Loop through each trial (row) in the verticalgrf_CMJ_data\n",
        "for trial_index in range(num_trials):\n",
        "    # Loop through each frame from 0 up to total_frames - 5\n",
        "    # The -5 is to ensure there are enough frames remaining to check for a sequence of 5 zeros\n",
        "    found_toe_off_for_trial = False\n",
        "    for frame_index in range(total_frames - 5):\n",
        "        # Check if the value at the current frame is exactly 0.000\n",
        "        if verticalgrf_CMJ_data[trial_index, frame_index] == 0.000:\n",
        "            # Check if the next four consecutive frames are also 0.000\n",
        "            if np.all(verticalgrf_CMJ_data[trial_index, frame_index : frame_index + 5] == 0.000):\n",
        "                # If a sequence of five consecutive 0.000 values is found, append the 1-based frame number\n",
        "                toe_off_frames.append(frame_index + 1)\n",
        "                found_toe_off_for_trial = True\n",
        "                break # Break out of the inner (frame) loop to move to the next trial\n",
        "    if not found_toe_off_for_trial:\n",
        "        toe_off_frames.append(np.nan) # Append NaN if no toe-off found for the trial\n",
        "\n",
        "# Calculate the average of the identified toe-off frames, ignoring NaN values\n",
        "average_toe_off_frame = np.nanmean(toe_off_frames)\n",
        "\n",
        "# Assign this average to the 'take_off' variable, converting to int if not NaN\n",
        "if not np.isnan(average_toe_off_frame):\n",
        "    cmj_TO = int(round(average_toe_off_frame))\n",
        "else:\n",
        "    cmj_TO = None # Or handle as appropriate if no valid frames were found\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "# FIND CMJ TOUCHDOWN TIME (LANDING) - After CMJ Take-off\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "\n",
        "touchdown_frames = [] # Initialize an empty list to store the touchdown (landing) frame for each CMJ trial\n",
        "\n",
        "if cmj_TO is not None: # Ensure cmj_TO is defined before searching for landing\n",
        "    for trial_index in range(num_trials):\n",
        "        found_touchdown_for_trial = False\n",
        "        # Start searching for touchdown AFTER the cmj_TO event\n",
        "        # cmj_TO is 1-based, so for 0-based array indexing, we start from cmj_TO.\n",
        "        for frame_index in range(cmj_TO, total_frames):\n",
        "            # Check if the GRF value is greater than 0.5 (indicating landing)\n",
        "            if verticalgrf_CMJ_data[trial_index, frame_index] > 0:\n",
        "                touchdown_frames.append(frame_index + 1) # Append 1-based frame number\n",
        "                found_touchdown_for_trial = True\n",
        "                break # Break out of the inner (frame) loop to move to the next trial\n",
        "        if not found_touchdown_for_trial:\n",
        "            touchdown_frames.append(np.nan) # Append NaN if no touchdown (landing) found for the trial\n",
        "else:\n",
        "    print(\"cmj_TO was not found, cannot determine CMJ touchdown (landing) frames.\")\n",
        "    touchdown_frames = [np.nan] * num_trials # Populate with NaNs if cmj_TO is missing\n",
        "\n",
        "# Calculate the average of the identified touchdown frames, ignoring NaN values\n",
        "average_touchdown_frame = np.nanmean(touchdown_frames)\n",
        "\n",
        "# Assign this average to the 'cmj_TD' variable, converting to int if not NaN\n",
        "if not np.isnan(average_touchdown_frame):\n",
        "    cmj_TD = int(round(average_touchdown_frame))-1\n",
        "else:\n",
        "    cmj_TD = None # Or handle as appropriate if no valid frames were found\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "# FIND SJ TAKE-OFF TIME\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "\n",
        "toe_off_frames = [] # re-initialize an empty list to store the toe-off frame for each SJ trial\n",
        "\n",
        "num_trials = len(verticalgrf_SJ_data) # Number of trials (rows) in the data\n",
        "total_frames = verticalgrf_SJ_data.shape[1] # Total number of frames (columns) in the data\n",
        "# Loop through each trial (row) in the verticalgrf_SJ_data\n",
        "for trial_index in range(num_trials):\n",
        "    # Loop through each frame from 0 up to total_frames - 5\n",
        "    # The -5 is to ensure there are enough frames remaining to check for a sequence of 5 zeros\n",
        "    found_toe_off_for_trial = False\n",
        "    for frame_index in range(total_frames - 5):\n",
        "        # Check if the value at the current frame is exactly 0.000\n",
        "        if verticalgrf_SJ_data[trial_index, frame_index] == 0.000:\n",
        "            # Check if the next four consecutive frames are also 0.000\n",
        "            if np.all(verticalgrf_SJ_data[trial_index, frame_index : frame_index + 5] == 0.000):\n",
        "                # If a sequence of five consecutive 0.000 values is found, append the 1-based frame number\n",
        "                toe_off_frames.append(frame_index + 1)\n",
        "                found_toe_off_for_trial = True\n",
        "                break # Break out of the inner (frame) loop to move to the next trial\n",
        "    if not found_toe_off_for_trial:\n",
        "        toe_off_frames.append(np.nan) # Append NaN if no toe-off found for the trial\n",
        "\n",
        "# Calculate the average of the identified toe-off frames, ignoring NaN values\n",
        "average_toe_off_frame = np.nanmean(toe_off_frames)\n",
        "\n",
        "# Assign this average to the 'take_off' variable, converting to int if not NaN\n",
        "if not np.isnan(average_toe_off_frame):\n",
        "    sj_TO = int(round(average_toe_off_frame))\n",
        "else:\n",
        "    sj_TO = None # Or handle as appropriate if no valid frames were found\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "# FIND SJ TOUCHDOWN TIME (LANDING) - After SJ Take-off\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "\n",
        "touchdown_frames = [] # Initialize an empty list to store the touchdown (landing) frame for each SJ trial\n",
        "\n",
        "if sj_TO is not None: # Ensure sj_TO is defined before searching for landing\n",
        "    for trial_index in range(num_trials):\n",
        "        found_touchdown_for_trial = False\n",
        "        # Start searching for touchdown AFTER the sj_TO event\n",
        "        # sj_TO is 1-based, so for 0-based array indexing, we start from sj_TO.\n",
        "        for frame_index in range(sj_TO, total_frames):\n",
        "            # Check if the GRF value is greater than 0.5 (indicating landing)\n",
        "            if verticalgrf_SJ_data[trial_index, frame_index] > 0:\n",
        "                touchdown_frames.append(frame_index + 1) # Append 1-based frame number\n",
        "                found_touchdown_for_trial = True\n",
        "                break # Break out of the inner (frame) loop to move to the next trial\n",
        "        if not found_touchdown_for_trial:\n",
        "            touchdown_frames.append(np.nan) # Append NaN if no touchdown (landing) found for the trial\n",
        "else:\n",
        "    print(\"sj_TO was not found, cannot determine SJ touchdown (landing) frames.\")\n",
        "    touchdown_frames = [np.nan] * num_trials # Populate with NaNs if sj_TO is missing\n",
        "\n",
        "# Calculate the average of the identified touchdown frames, ignoring NaN values\n",
        "average_touchdown_frame = np.nanmean(touchdown_frames)\n",
        "\n",
        "# Assign this average to the 'sj_TD' variable, converting to int if not NaN\n",
        "if not np.isnan(average_touchdown_frame):\n",
        "    sj_TD = int(round(average_touchdown_frame))-1\n",
        "else:\n",
        "    sj_TD = None # Or handle as appropriate if no valid frames were found\n",
        "\n",
        "\n",
        "print(f\"Assigned 'cmj_TO' variable: {cmj_TO}\")\n",
        "print(f\"Assigned 'cmj_TD' variable: {cmj_TD}\")\n",
        "print(f\"Assigned 'sj_TO' variable: {sj_TO}\")\n",
        "print(f\"Assigned 'sj_TD' variable: {sj_TD}\")\n",
        "print(\"Shape of Vertical_GRF (CMJ) data:\", verticalgrf_CMJ_data.shape)\n",
        "print(\"Shape of Vertical_GRF (SJ) data:\", verticalgrf_SJ_data.shape)\n",
        "print(\"Shape of Hamstring EMG (CMJ) data:\", hamstring_CMJ_data.shape)\n",
        "print(\"Shape of Hamstring EMG (SJ) data:\", hamstring_SJ_data.shape)\n",
        "print(\"Shape of Rectus EMG (CMJ) data:\", rectus_CMJ_data.shape)\n",
        "print(\"Shape of Rectus EMG (SJ) data:\", rectus_SJ_data.shape)\n",
        "print(\"Shape of Gastroc EMG (CMJ) data:\", gastroc_CMJ_data.shape)\n",
        "print(\"Shape of Gastroc EMG (SJ) data:\", gastroc_SJ_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPM Paired t-test\n",
        "\n",
        "Now let's perform the SPM paired t-tests on the time-series data in this lab so that we can compare them between conditions (CMJ vs. SJ). The specific code below produces the ensemble-averaged plots for both conditions with shaded bands representing suprathreshold regions where the mean differences between condition are statistically significant (SPM{t} p < 0.05).\n",
        "\n",
        "### âœ‹ GenAI prompt:\n",
        "\n",
        "Produce code that will compare the time-series data wrangled earlier between CMJ and SJ conditions using SPM paired t-tests. Be sure to plot the ensemble-average curves for each variable and condition along with the SPM{t} field. Do not output the SPM{t} fields. Rather highlight the suprathreshold regions with shaded bands.\n",
        "\n",
        "### âœ… Pre-compiled code:"
      ],
      "metadata": {
        "id": "BREANsZJaOc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spm1d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "total_frames = 101\n",
        "num_ticks = 6  # Number of ticks (0%, 10%, ..., 100%)\n",
        "\n",
        "# Define a function to create each plot\n",
        "def plot_spm(ax, Y1, Y2, label_Y1, label_Y2, take_off1, take_off2, land1, land2, x_label, y_label, y_limits, units, color_Y1, color_Y2, show_legend):\n",
        "    spm1d.plot.plot_mean_sd(Y1, label=label_Y1, linecolor=color_Y1, facecolor=color_Y1, ax=ax, lw=1.5, alpha=0.3, autoset_ylim=False)\n",
        "    spm1d.plot.plot_mean_sd(Y2, label=label_Y2, linecolor=color_Y2, facecolor=color_Y2, linestyle='--', lw=1.5, alpha=0.4, ax=ax, autoset_ylim=False)\n",
        "    ax.axhline(y=0, color='k', linestyle=':')\n",
        "\n",
        "    # --- Start of fix for zero variance error ---\n",
        "    # Check for zero variance in Y1 and Y2 at each node and add tiny noise if variance is zero\n",
        "    Y1_copy = Y1.copy() # Create copies to avoid modifying original global arrays directly\n",
        "    Y2_copy = Y2.copy()\n",
        "    for i in range(Y1_copy.shape[1]): # Iterate through each node (column)\n",
        "        if np.var(Y1_copy[:, i]) == 0:\n",
        "            Y1_copy[:, i] += np.random.rand(Y1_copy.shape[0]) * 1e-9 # Add tiny noise if variance is zero\n",
        "        if np.var(Y2_copy[:, i]) == 0:\n",
        "            Y2_copy[:, i] += np.random.rand(Y2_copy.shape[0]) * 1e-9 # Add tiny noise if variance is zero\n",
        "    # --- End of fix ---\n",
        "\n",
        "    # Conduct t-test (using ttest_paired for paired comparison):\n",
        "    alpha      = 0.05\n",
        "    t          = spm1d.stats.ttest_paired(Y1_copy, Y2_copy) # Pass Y1_copy and Y2_copy separately\n",
        "    ti         = t.inference(alpha, two_tailed=True, interp=True)\n",
        "\n",
        "    # Highlight suprathreshold regions\n",
        "    for cluster in ti.clusters:\n",
        "        start, end = cluster.endpoints\n",
        "        ax.axvspan(start, end, ymin=0, ymax=1.0, color='grey', alpha=0.20)\n",
        "\n",
        "    # Generate plots\n",
        "    percentages = np.linspace(0, 100, num_ticks)  # 0%, 10%, ..., 100%\n",
        "    x_ticks = (percentages / 100) * total_frames  # Normalize frame numbers\n",
        "    ypos = y_limits[1] + y_limits[1]*.02          # event labels\n",
        "    ax.set_xticklabels(percentages.astype(int), fontsize=8)  # Set labels as integers (0, 10%, ..., 100%)\n",
        "    # ax.set_xticks(x_ticks)\n",
        "    ax.vlines(x=take_off2, ymin=y_limits[0]-100, ymax=y_limits[1]+100, color='r', linestyle='--', lw=1)\n",
        "    ax.text(take_off2-2, ypos, 'TO', fontsize=8, color='r')\n",
        "    ax.vlines(x=take_off1, ymin=y_limits[0]-100, ymax=y_limits[1]+100, color='k', linestyle='-', lw=1)\n",
        "    ax.text(take_off1-2, ypos, 'TO', fontsize=8)\n",
        "    ax.vlines(x=land2, ymin=y_limits[0]-100, ymax=y_limits[1]+100, color='r', linestyle='--', lw=1)\n",
        "    ax.text(land2-2, ypos, 'TD', fontsize=8, color='r')\n",
        "    ax.vlines(x=land1, ymin=y_limits[0]-100, ymax=y_limits[1]+100, color='k', linestyle='-', lw=1)\n",
        "    ax.text(land1-2, ypos, 'TD', fontsize=8)\n",
        "    ax.set_xlabel(x_label, fontsize=10, fontweight='bold')\n",
        "    ax.set_ylabel(y_label, fontsize=10, fontweight='bold') if y_label else ax.set_ylabel(\"\")\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(y_limits)\n",
        "\n",
        "    if show_legend:\n",
        "      ax.legend(fontsize=9, loc='upper right', frameon=False)\n",
        "\n",
        "# Combine both SPM plots (no SPM{t} fields)\n",
        "pyplot.figure( figsize=(10, 9) ) # Adjusted figsize for a 4 plot grid\n",
        "\n",
        "# =============================================================================\n",
        "# Vertical GRF\n",
        "# =============================================================================\n",
        "\n",
        "# Replace x and y axes and filename label\n",
        "xlabel = \"% Jump Motion\"\n",
        "ylabel=\"Vertical GRF\"\n",
        "units = \"N\"\n",
        "\n",
        "# YA (black)\n",
        "YA            = verticalgrf_CMJ_data\n",
        "\n",
        "# YB (red)\n",
        "YB            = verticalgrf_SJ_data\n",
        "\n",
        "# Calculate shared y-axis limits\n",
        "all_data_for_y_limits = np.concatenate((YA.flatten(), YB.flatten()))\n",
        "all_data_for_y_limits = all_data_for_y_limits[~np.isnan(all_data_for_y_limits)] # remove NaNs\n",
        "\n",
        "if len(all_data_for_y_limits) > 0:\n",
        "    min_val = np.min(all_data_for_y_limits)\n",
        "    max_val = np.max(all_data_for_y_limits)\n",
        "    if min_val > 0:\n",
        "        min_val = 0\n",
        "else: # If all data is NaN after concatenation and filtering\n",
        "    min_val = -1.0 # Default to some reasonable range if all data is NaN\n",
        "    max_val = 1.0\n",
        "y_limits = [min_val, max_val]\n",
        "\n",
        "# Subplot 1\n",
        "ax1 = pyplot.subplot(2, 2, 1) # Changed to 1 row, 3 columns, position 1\n",
        "plot_spm(\n",
        "    ax1, YA, YB,\n",
        "    label_Y1=\"CMJ\",\n",
        "    label_Y2=\"SJ\",\n",
        "    take_off1 = cmj_TO,\n",
        "    take_off2 = sj_TO,\n",
        "    land1 = cmj_TD,\n",
        "    land2 = sj_TD,\n",
        "    x_label = xlabel,\n",
        "    y_label=f'{ylabel} ({units})',\n",
        "    y_limits=y_limits,\n",
        "    units=units,\n",
        "    color_Y1='k',\n",
        "    color_Y2='r',\n",
        "    show_legend=False\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Hamstring EMG\n",
        "# =============================================================================\n",
        "\n",
        "# Replace x and y axes and filename label\n",
        "# xlabel = \"% Jump Motion\"\n",
        "ylabel=\"Hamstring\"\n",
        "units = \"%MVC\"\n",
        "\n",
        "# YA (black)\n",
        "YA            = hamstring_CMJ_data\n",
        "\n",
        "# YB (red)\n",
        "YB            = hamstring_SJ_data\n",
        "\n",
        "# Calculate shared y-axis limits\n",
        "all_data_for_y_limits = np.concatenate((YA.flatten(), YB.flatten()))\n",
        "all_data_for_y_limits = all_data_for_y_limits[~np.isnan(all_data_for_y_limits)] # remove NaNs\n",
        "\n",
        "if len(all_data_for_y_limits) > 0:\n",
        "    min_val = np.min(all_data_for_y_limits)\n",
        "    max_val = np.max(all_data_for_y_limits)\n",
        "    if min_val > 0:\n",
        "        min_val = 0\n",
        "else: # If all data is NaN after concatenation and filtering\n",
        "    min_val = -1.0 # Default to some reasonable range if all data is NaN\n",
        "    max_val = 1.0\n",
        "\n",
        "# min_val = 0 # Removed hardcoded limits\n",
        "# max_val = 1000 # Removed hardcoded limits\n",
        "y_limits = [min_val, max_val] # Use dynamically calculated limits\n",
        "\n",
        "# Subplot 2\n",
        "ax2 = pyplot.subplot(2, 2, 2) # Changed to 1 row, 3 columns, position 2\n",
        "plot_spm(\n",
        "    ax2, YA, YB,\n",
        "    label_Y1=\"CMJ\",\n",
        "    label_Y2=\"SJ\",\n",
        "    take_off1 = cmj_TO,\n",
        "    take_off2 = sj_TO,\n",
        "    land1 = cmj_TD,\n",
        "    land2 = sj_TD,\n",
        "    x_label = xlabel,\n",
        "    y_label=f'{ylabel} ({units})',\n",
        "    y_limits=y_limits,\n",
        "    units=units,\n",
        "    color_Y1='k',\n",
        "    color_Y2='r',\n",
        "    show_legend=True\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Rectus EMG\n",
        "# =============================================================================\n",
        "\n",
        "# Replace x and y axes and filename label\n",
        "# xlabel = \"% Jump Motion\"\n",
        "ylabel=\"Rectus EMG\"\n",
        "units = \"%MVC\"\n",
        "\n",
        "# YA (black)\n",
        "YA            = rectus_CMJ_data\n",
        "\n",
        "# YB            = rectus_SJ_data # This line was commented out in previous execution, uncommenting it for correct execution\n",
        "\n",
        "YB            = rectus_SJ_data\n",
        "\n",
        "# Calculate shared y-axis limits\n",
        "all_data_for_y_limits = np.concatenate((YA.flatten(), YB.flatten()))\n",
        "all_data_for_y_limits = all_data_for_y_limits[~np.isnan(all_data_for_y_limits)] # remove NaNs\n",
        "\n",
        "if len(all_data_for_y_limits) > 0:\n",
        "    min_val = np.min(all_data_for_y_limits)\n",
        "    max_val = np.max(all_data_for_y_limits)\n",
        "    if min_val > 0:\n",
        "        min_val = 0\n",
        "else: # If all data is NaN after concatenation and filtering\n",
        "    min_val = -1.0 # Default to some reasonable range if all data is NaN\n",
        "    max_val = 1.0\n",
        "\n",
        "# min_val = 0 # Removed hardcoded limits\n",
        "# max_val = 1000 # Removed hardcoded limits\n",
        "y_limits = [min_val, max_val] # Use dynamically calculated limits\n",
        "\n",
        "# Subplot 3\n",
        "ax3 = pyplot.subplot(2, 2, 3) # Changed to 2 rows, 2 columns, position 1\n",
        "plot_spm(\n",
        "    ax3, YA, YB,\n",
        "    label_Y1=\"CMJ\",\n",
        "    label_Y2=\"SJ\",\n",
        "    take_off1 = cmj_TO,\n",
        "    take_off2 = sj_TO,\n",
        "    land1 = cmj_TD,\n",
        "    land2 = sj_TD,\n",
        "    x_label = xlabel,\n",
        "    y_label=f'{ylabel} ({units})',\n",
        "    y_limits=y_limits,\n",
        "    units=units,\n",
        "    color_Y1='k',\n",
        "    color_Y2='r',\n",
        "    show_legend=False\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Gastroc EMG\n",
        "# =============================================================================\n",
        "\n",
        "# Replace x and y axes and filename label\n",
        "# xlabel = \"% Jump Motion\"\n",
        "ylabel=\"Gastroc EMG\"\n",
        "units = \"%MVC\"\n",
        "\n",
        "# YA (black)\n",
        "YA            = gastroc_CMJ_data\n",
        "\n",
        "# YB (red)\n",
        "YB            = gastroc_SJ_data\n",
        "\n",
        "# Calculate shared y-axis limits\n",
        "all_data_for_y_limits = np.concatenate((YA.flatten(), YB.flatten()))\n",
        "all_data_for_y_limits = all_data_for_y_limits[~np.isnan(all_data_for_y_limits)] # remove NaNs\n",
        "\n",
        "if len(all_data_for_y_limits) > 0:\n",
        "    min_val = np.min(all_data_for_y_limits)\n",
        "    max_val = np.max(all_data_for_y_limits)\n",
        "    if min_val > 0:\n",
        "        min_val = 0\n",
        "else: # If all data is NaN after concatenation and filtering\n",
        "    min_val = -1.0 # Default to some reasonable range if all data is NaN\n",
        "    max_val = 1.0\n",
        "\n",
        "# min_val = 0 # Removed hardcoded limits\n",
        "# max_val = 1000 # Removed hardcoded limits\n",
        "y_limits = [min_val, max_val] # Use dynamically calculated limits\n",
        "\n",
        "# Subplot 4\n",
        "ax4 = pyplot.subplot(2, 2, 4) # Changed to 2 rows, 2 columns, position 1, now ax4\n",
        "plot_spm(\n",
        "    ax4, YA, YB,\n",
        "    label_Y1=\"CMJ\",\n",
        "    label_Y2=\"SJ\",\n",
        "    take_off1 = cmj_TO,\n",
        "    take_off2 = sj_TO,\n",
        "    land1 = cmj_TD,\n",
        "    land2 = sj_TD,\n",
        "    x_label = xlabel,\n",
        "    y_label=f'{ylabel} ({units})',\n",
        "    y_limits=y_limits,\n",
        "    units=units,\n",
        "    color_Y1='k',\n",
        "    color_Y2='r',\n",
        "    show_legend=False\n",
        ")\n",
        "\n",
        "### plot SPM results:\n",
        "pyplot.tight_layout()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "Cs1lgmj92ddQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd4c6d2a"
      },
      "source": [
        "\n",
        "# Summary Statistics\n",
        "\n",
        "Now the discrete metrics (e.g., stride length, speed, etc.) need to be compared between conditions using traditional paired t-tests with the summary statistics displayed in an APA-formatted table.\n",
        "\n",
        "### âœ‹ GenAI prompt:\n",
        "\n",
        "Analyze the `discretes_df` to compare CMJ and SJ conditions. This involves preparing the data by extracting a unique subject identifier, calculating summary statistics (mean, standard deviation, count) for the GRF and EMG metrics grouped by condition, and performing paired t-tests for these variables between the two conditions. Finally, format these results into an APA-style table and summarize the key findings, highlighting any statistically significant differences.\n",
        "\n",
        "### âœ… Pre-compiled code:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4a9be9"
      },
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# 1. Create a copy of discretes_df\n",
        "discretes_for_analysis_df = discretes_df.copy()\n",
        "\n",
        "# 2. Extract a unique subject identifier (the numerical part of the filename) from the 'filename' column\n",
        "# Assuming filenames are like 'GROUP_CONDITION_SUBJECTID.txt' (e.g., Martell_TRN_04.txt -> 04)\n",
        "discretes_for_analysis_df['trial_id'] = discretes_for_analysis_df['filename'].str.extract(r'_(\\d+)\\.txt$')[0]\n",
        "\n",
        "# Define the desired column order\n",
        "# This will effectively drop 'filename' and place 'trial_id' after 'condition'\n",
        "final_columns = ['condition', 'trial_id'] + [col for col in discretes_for_analysis_df.columns if col not in ['condition', 'trial_id', 'filename']]\n",
        "discretes_for_analysis_df = discretes_for_analysis_df[final_columns]\n",
        "\n",
        "# Sort the DataFrame by 'condition' to ensure 'CMJ' comes before 'SJ'\n",
        "# This uses 'CMJ' and 'SJ' as categories with a specific order\n",
        "discretes_for_analysis_df['condition'] = pd.Categorical(\n",
        "    discretes_for_analysis_df['condition'],\n",
        "    categories=['CMJ', 'SJ'],\n",
        "    ordered=True\n",
        ")\n",
        "discretes_for_analysis_df = discretes_for_analysis_df.sort_values(by=['condition', 'trial_id']).reset_index(drop=True)\n",
        "\n",
        "# 3. Display the head of the discretes_for_analysis_df to verify the new 'trial_id' column and sorting\n",
        "print(\"Head of discretes_for_analysis_df with new 'trial_id' column and sorted conditions:\")\n",
        "print(discretes_for_analysis_df.head())\n",
        "\n",
        "# 4. Define a list of the variables for which summary statistics need to be calculated\n",
        "summary_variables = [\n",
        "        'Jump_Height_cm',\n",
        "        'PEAK_GRFz'\n",
        "]\n",
        "\n",
        "# 5. Group the discretes_for_analysis_df DataFrame by the 'condition' column\n",
        "# 6. For each of the specified variables, calculate the mean, standard deviation, and count for each 'condition' group\n",
        "summary_statistics = discretes_for_analysis_df.groupby('condition')[summary_variables].agg(\n",
        "    ['mean', 'std', 'count']\n",
        ")\n",
        "\n",
        "# 7. Store these summary statistics in a new DataFrame (already done by the agg function)\n",
        "# The result is automatically a DataFrame with a multi-level index/columns which is well-organized.\n",
        "\n",
        "# 8. Display the resulting summary statistics DataFrame\n",
        "print(\"Descriptive Statistics grouped by Condition:\")\n",
        "print(summary_statistics)\n",
        "\n",
        "# 9. Define a list of the variables for which paired t-tests need to be performed\n",
        "pair_ttest_variables = [\n",
        "        'Jump_Height_cm',\n",
        "        'PEAK_GRFz'\n",
        "]\n",
        "\n",
        "# 10. Initialize a dictionary to store the t-test results\n",
        "t_test_results = {}\n",
        "\n",
        "# 11. For each variable in the defined list:\n",
        "for var in pair_ttest_variables:\n",
        "    # a. Filter discretes_for_analysis_df to get data for the 'CMJ' condition.\n",
        "    CMJ_data = discretes_for_analysis_df[discretes_for_analysis_df['condition'] == 'CMJ'][['trial_id', var]]\n",
        "\n",
        "    # b. Filter discretes_for_analysis_df to get data for the 'SJ' condition.\n",
        "    SJ_data = discretes_for_analysis_df[discretes_for_analysis_df['condition'] == 'SJ'][['trial_id', var]]\n",
        "\n",
        "    # c. Ensure that both filtered DataFrames contain the same 'trial_id's and are sorted by 'trial_id'\n",
        "    # to guarantee correct pairing.\n",
        "    merged_data = pd.merge(CMJ_data, SJ_data, on='trial_id', suffixes=('_CMJ', '_SJ'))\n",
        "\n",
        "    # Drop rows where either CMJ or SJ data is NaN for the current variable\n",
        "    merged_data = merged_data.dropna(subset=[var + '_CMJ', var + '_SJ'])\n",
        "\n",
        "    # Extract the values for the current variable from both filtered DataFrames.\n",
        "    data_CMJ = merged_data[var + '_CMJ']\n",
        "    data_SJ = merged_data[var + '_SJ']\n",
        "\n",
        "    # d. Perform a paired t-test using ttest_rel on these two sets of values.\n",
        "    # Check if there are enough samples to perform the t-test\n",
        "    if len(data_CMJ) >= 2 and len(data_SJ) >= 2:\n",
        "        t_statistic, p_value = ttest_rel(data_CMJ, data_SJ)\n",
        "    else:\n",
        "        t_statistic, p_value = float('nan'), float('nan') # Set to NaN if not enough samples\n",
        "\n",
        "    # e. Store the resulting t-statistic and p-value.\n",
        "    t_test_results[var] = {'t_statistic': t_statistic, 'p_value': p_value}\n",
        "\n",
        "# 12. Print or display the collected t-test results for each variable.\n",
        "print(\"\\nPaired t-test results (CMJ vs. SJ):\")\n",
        "for var, results in t_test_results.items():\n",
        "    print(f\"Variable: {var}\")\n",
        "    print(f\"  t-statistic: {results['t_statistic']:.3f}\")\n",
        "    print(f\"  p-value: {results['p_value']:.3f}\")\n",
        "    print(\"-----------------------------------------\")\n",
        "# 13. Initialize an empty list to store the data for the APA-style table\n",
        "apa_table_data = []\n",
        "\n",
        "# 14. Iterate through each variable in the pair_ttest_variables list\n",
        "for var in pair_ttest_variables:\n",
        "    # 3. For each variable, extract the mean and standard deviation for 'CMJ' and 'SJ' conditions\n",
        "    #    from the summary_statistics DataFrame.\n",
        "    mean_CMJ = summary_statistics.loc['CMJ', (var, 'mean')]\n",
        "    std_CMJ = summary_statistics.loc['CMJ', (var, 'std')]\n",
        "    mean_SJ = summary_statistics.loc['SJ', (var, 'mean')]\n",
        "    std_SJ = summary_statistics.loc['SJ', (var, 'std')]\n",
        "\n",
        "    # 4. Extract the t-statistic and p-value for the current variable from the t_test_results dictionary.\n",
        "    t_stat = t_test_results[var]['t_statistic']\n",
        "    p_val = t_test_results[var]['p_value']\n",
        "\n",
        "    # 5. Create a dictionary for the current row with appropriate keys and formatted numerical values.\n",
        "    row = {\n",
        "        'Variable': var,\n",
        "        'M \\u00b1 SD (CMJ)': f\"{mean_CMJ:.2f} \\u00b1 {std_CMJ:.2f}\",\n",
        "         # 'SD (CMJ)': f\"{std_CMJ:.2f}\",\n",
        "        'M \\u00b1 SD (SJ)': f\"{mean_SJ:.2f} \\u00b1 {std_SJ:.2f}\",\n",
        "         # 'SD (SJ)': f\"{std_SJ:.2f}\",\n",
        "         # 't': f\"{t_stat:.3f}\",\n",
        "        'p': f\"{p_val:.3f}\"\n",
        "    }\n",
        "\n",
        "    # Append this row dictionary to the list.\n",
        "    apa_table_data.append(row)\n",
        "\n",
        "# 15. Create a pandas DataFrame from the list of rows.\n",
        "apa_table_df = pd.DataFrame(apa_table_data)\n",
        "\n",
        "print(\"APA-Style Summary and Paired t-test Results Table:\")\n",
        "print(apa_table_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ‘ Complete the Analysis\n",
        "\n",
        "After you've completed this lab's analysis, do the following:\n",
        "\n",
        "1. Copy and paste the above plots to your lab report\n",
        "2. Export the tables using the code in the following cell. You can download them from your runtime directory onto your own computer.\n",
        "2. Share your Colab notebook with me\n",
        "3. Copy the link to your notebook and submit it along with your report for this lab on Canvas"
      ],
      "metadata": {
        "id": "W0vQo8A1VFOX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08cf6c31"
      },
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Construct the full file path for the CSV file\n",
        "output_filepath = os.path.join(zip_dir, f\"{GROUP}_discretes_for_analysis_lab6.csv\")\n",
        "\n",
        "# Export the DataFrame to CSV\n",
        "discretes_for_analysis_df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"'discretes_for_analysis_df' exported to '{output_filepath}'\")\n",
        "\n",
        "# Construct the full file path for the CSV file\n",
        "output_filepath = os.path.join(zip_dir, f\"{GROUP}_summary_stats_lab6.csv\")\n",
        "\n",
        "# Export the APA formatted summary DataFrame to CSV\n",
        "apa_table_df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"'summary_statistics' exported to '{output_filepath}'\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}